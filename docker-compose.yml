x-airflow-env: &airflow_env
  AIRFLOW__CORE__LOAD_EXAMPLES: "false"
  AIRFLOW__CORE__EXECUTOR: "LocalExecutor"
  AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
  AIRFLOW__SCHEDULER__CATCHUP_BY_DEFAULT: "false"
  AIRFLOW__CORE__DEFAULT_TIMEZONE: "America/Sao_Paulo"
  AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: "postgresql+psycopg2://airflow:airflow@airflow_db:5432/airflow"
  AIRFLOW__CORE__FERNET_KEY: "Y1nJb3mWc7e3x1bqVm3y3YF4n1sR1y1i2lV4H0V0B5E="  # chave de exemplo p/ o case
  AIRFLOW__API__AUTH_BACKENDS: "airflow.api.auth.backend.basic_auth"
  _PIP_ADDITIONAL_REQUIREMENTS: ""

services:
  dw_db:
    image: postgres:16
    container_name: dw_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: dw
      POSTGRES_USER: dw_user
      POSTGRES_PASSWORD: dw_password
    ports:
      - "5433:5432"
    volumes:
      - dw_data:/var/lib/postgresql/data
    networks:
      - banvic_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U dw_user -d dw"]
      interval: 5s
      timeout: 3s
      retries: 20

  source_db:
    image: postgres:16
    container_name: source_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: banvic
      POSTGRES_USER: data_engineer
      POSTGRES_PASSWORD: "v3rysecur&pas5w0rd"   # aspas por causa do &
    volumes:
      - source_db_data:/var/lib/postgresql/data
      - ./source/banvic.sql:/docker-entrypoint-initdb.d/banvic.sql
    ports:
      - "55432:5432"
    networks:
      - banvic_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U data_engineer -d banvic"]
      interval: 5s
      timeout: 3s
      retries: 20

  airflow_db:
    image: postgres:16
    container_name: airflow_db
    restart: unless-stopped
    environment:
      POSTGRES_DB: airflow
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
    ports:
      - "5434:5432"
    volumes:
      - airflow_db_data:/var/lib/postgresql/data
    networks:
      - banvic_net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U airflow -d airflow"]
      interval: 5s
      timeout: 3s
      retries: 20

  airflow:
    image: apache/airflow:2.9.1
    container_name: airflow
    restart: unless-stopped
    depends_on:
      airflow_db:
        condition: service_healthy
      dw_db:
        condition: service_healthy
      source_db:
        condition: service_healthy
    environment:
      <<: *airflow_env
      AIRFLOW_UID: "50000"
      AIRFLOW_GID: "0"

      SOURCE_DB_HOST: source_db
      SOURCE_DB_PORT: "5432"
      SOURCE_DB_USER: "data_engineer"
      SOURCE_DB_PASSWORD: "v3rysecur&pas5w0rd"
      SOURCE_DB_NAME: "banvic"

      DW_DB_HOST: dw_db
      DW_DB_PORT: "5432"
      DW_DB_USER: "dw_user"
      DW_DB_PASSWORD: "dw_password"
      DW_DB_NAME: "dw"

      AIRFLOW_USER: "airflow"
      AIRFLOW_PASS: "airflow"
      AIRFLOW_EMAIL: "admin@example.com"

    command: >
      bash -c "
      pip install -r /opt/airflow/requirements.txt &&
      airflow db init &&
      airflow users create
        --username ${AIRFLOW_USER}
        --password ${AIRFLOW_PASS}
        --firstname Air
        --lastname Flow
        --role Admin
        --email ${AIRFLOW_EMAIL} || true &&
      airflow webserver & airflow scheduler
      "
    ports:
      - "8080:8080"
    volumes:
      - ./dags:/opt/airflow/dags
      - ./requirements.txt:/opt/airflow/requirements.txt
      - ./data:/opt/airflow/data
      - ./datasets:/opt/airflow/datasets
      - ./logs:/opt/airflow/logs
      - ./plugins:/opt/airflow/plugins
    networks:
      - banvic_net

networks:
  banvic_net:

volumes:
  dw_data:
  source_db_data:
  airflow_db_data: